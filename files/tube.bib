
@book{burtner,
	title = {Metasaxophone {Systems}: {Evolving} {New} {Music} and {Technologies} for the {Saxophone}},
	url = {https://ccrma.stanford.edu/ mburtner/metasax.html#Overview},
	language = {English},
	urldate = {2019-12-05},
	author = {Burtner, Matthew}
}

@inproceedings{bowers2005,
	address = {Vancouver, BC, Canada},
	title = {Not {Hyper}, {Not} {Meta}, {Not} {Cyber} but {Infra}-{Instruments}},
	url = {https://www.nime.org/proceedings/2005/nime2005_005.pdf},
	language = {English},
	urldate = {2020-04-20},
	author = {Bowers, John and Archer, Phil},
	year = {2005},
	pages = {5--10},
	file = {Full Text:/Users/Vincent/Zotero/storage/LAXTIIID/Bowers and Archer - 2005 - Not Hyper, Not Meta, Not Cyber but Infra-Instrumen.pdf:application/pdf}
}

@inproceedings{machover1989,
	address = {San Francisco, CA},
	title = {Hyperinstruments: {Musically} {Intelligent} and {Interactive} {Performance} and {Creativity} {Systems}},
	language = {English},
	author = {Machover, Tod and Chung, Joe},
	year = {1989},
	pages = {186--190}
}

@inproceedings{schiesser2006,
	address = {Paris, France},
	title = {On making and playing an electronically-augmented saxophone},
	language = {English},
	booktitle = {Proceedings of the 2006 {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	author = {Schiesser, Sébastien and Traube, Caroline},
	year = {2006},
	pages = {308--313}
}

@article{burtner2002,
	title = {The {Metasaxophone}: {Concept}, implementation, and mapping strategies for a new computer music instrument},
	volume = {7},
	issn = {1355-7718; 1469-8153 (electronic)},
	url = {https://proxy.library.mcgill.ca/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=rft&AN=A110582&scope=site},
	abstract = {The Metasaxophone is an acoustic tenor saxophone retrofitted with an onboard computer microprocessor and an array of sensors that convert performance data into MIDI control messages. The instrument has additionally been outfitted with a unique microphone system that allows for detailed control of the amplified sound. While maintaining full acoustic functionality it is also a versatile MIDI controller and an electric instrument. A primary motivation behind the Metasaxophone is to put signal processing under direct expressive control of the performer. Through the combination of gestural and audio performance control, employing both discrete and continuous multilayered mapping strategies, the Metasaxophone can be adapted for a wide range of musical purposes. The artistic and technical development of the instrument, as well as new conceptions of musical mappings arising from the enhanced interface, are explored.},
	number = {2},
	journal = {Organised sound: An international journal of music technology. VII/2 (August 2002): Mapping strategies in realtime computer music},
	author = {Burtner, Matthew},
	month = aug,
	year = {2002},
	keywords = {48: Sound sources – Electrophones (synthesized sound), electronic sound generation – mapping strategies – Metasaxophone, instruments—wind (woodwind, reed family) – saxophone – Metasaxophone},
	pages = {201--213}
}

@inproceedings{palacio-quintin2003,
	address = {Montreal, Canada},
	title = {The {Hyper}-{Flute}},
	language = {English},
	booktitle = {Proceedings of the 2003 {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	author = {Palacio-Quintin, Cléo},
	year = {2003},
	pages = {206--207}
}

@inproceedings{aska2014,
	title = {The {Black} {Swan}: {Probable} and {Improbable} {Communcation} {Over} {Local} and {Geographically} {Displaced} {Networked} {Connections} as a {Musical} {Performance} {System}},
	shorttitle = {The {Black} {Swan}},
	abstract = {The Black Swan is a networked performance system for two groups of non-specific performers. The work derives its title and inspiration from Nicolas Taleb's description of extreme and catastrophic events. These “black swan” events are characterized as being outliers, unpredictable, and yet completely explainable when viewed in retrospect. The Black Swan uses this concept in performance; throughout the piece a group of instrumentalists is solely responsible for interpreting the score while a group of motion-tracked performers advance the score. However, when the “Black Swan” occurs, the motion-tracked group begins to generate sound, an event that the instrumentalists could not have anticipated. A third party is responsible for distributing instructions to each performance group over the network during the performance. Therefore, The Black Swan explores the way networked performers communicate with each other as well as the dramaturgy between ensemble members in a networked setting.},
	booktitle = {{ICMC}},
	author = {Aska, A.},
	year = {2014}
}

@book{aska2016,
	title = {{IMPROVISATION} {AND} {GESTURE} {AS} {FORM} {DETERMINANTS} {IN} {WORKS} {WITH} {ELECTRONICS}},
	abstract = {This paper examines several examples that use electronics as form determinants in works with some degree of structured improvisation. Three works created by the author are discussed, each of which uses gestural controller input to realize an indeterminate form in some way. The application of such principles and systems to venues such as networked performance is explored. While each of these works contains an improvisatory and/or aleatoric element, much of their content is composed, which brings the role of the composer into question. The “improviser”, who in these works advances the work temporally and determines the overall form, is actually taking on the more familiar role of the conductor. Therefore, these works also bring up important conversation topics regarding performance practice in works that contain electronics and how they are realized.},
	language = {en},
	author = {Aska, A.},
	year = {2016}
}

@book{aspromallis2016,
	title = {Form-{Aware}, {Real}-{Time} {Adaptive} {Music} {Generation} for {Interactive} {Experiences} {Data}},
	abstract = {This archive contains data in support of the Sound and Music Computing 2016 paper: “FORM-AWARE, REAL-TIME ADAPTIVE MUSIC GENERATION FOR INTERACTIVE EXPERIENCES”.},
	language = {en},
	author = {Aspromallis, C. and Gold, Ne},
	year = {2016}
}

@inproceedings{assayag2010,
	title = {Interaction with {Machine} {Improvisation}},
	doi = {10.1007/978-3-642-12337-5_10},
	abstract = {We describe two multi-agent architectures for an improvisation oriented musician-machine interaction systems that learn in real time from human performers. The improvisation kernel is based on sequence modeling and statistical learning. We present two frameworks of interaction with this kernel. In the first, the stylistic interaction is guided by a human operator in front of an interactive computer environment. In the second framework, the stylistic interaction is delegated to machine intelligence and therefore, knowledge propagation and decision are taken care of by the computer alone. The first framework involves a hybrid architecture using two popular composition/performance environments, Max and OpenMusic, that are put to work and communicate together, each one handling the process at a different time/memory scale. The second framework shares the same representational schemes with the first but uses an Active Learning architecture based on collaborative, competitive and memory-based learning to handle stylistic interactions. Both systems are capable of processing real-time audio/video as well as MIDI. After discussing the general cognitive background of improvisation practices, the statistical modelling tools and the concurrent agent architecture are presented. Then, an Active Learning scheme is described and considered in terms of using different improvisation regimes for improvisation planning. Finally, we provide more details about the different system implementations and describe several performances with the system.},
	booktitle = {The {Structure} of {Style}},
	author = {Assayag, G. and Bloch, G. and Cont, A. and Dubnov, S.},
	year = {2010},
	file = {Full Text:/Users/Vincent/Zotero/storage/46EM3K5S/Assayag et al. - 2010 - Interaction with Machine Improvisation.pdf:application/pdf}
}

@inproceedings{caetano2017,
	title = {A {Sensor}-{Augmented} {Saxophone} {Mouthpiece} for {Unveiling} the {Mechanics} of {Saxophone} {Tone} {Formation}},
	abstract = {This paper presents a sensor-augmented saxophone mouthpiece which promotes data collection from musicians. The collected data aims to unveil the mechanics of saxophone tone formation from the em-bouchure and air flow control— for which only limited knowledge exists due to its occlusion concealed by the players' mouth. Ultimately, by grasping and modeling how musicians play, we aim to develop intelligent music tutoring systems. Towards this aim, we provide the first steps of a sensor-augmented mouthpiece from which we compute air pressure, air flow, and mouthpiece-reed tip opening indicators of a performance. Di\${\textbackslash}textbackslashcarriagereturn\$erent strategies to capture these indicators have been studied under controlled lab conditions as a basis to develop a prototype mouthpiece which is evaluated by an expert saxophonist.},
	author = {Caetano, Gustavo and Bernardes, Gilberto and Twillert, Henk and Pais Clemente, Miguel and Mendes, Joaquim},
	month = sep,
	year = {2017}
}

@inproceedings{carey2012,
	address = {Ann Arbor, Michigan},
	title = {Designing for {Cumulative} {Interactivity}: {The} \_derivations {System}},
	doi = {10.5281/zenodo.1178227},
	abstract = {This paper presents the author's derivations system, an interactive performance system for solo improvising instrumentalist. The system makes use of a combination of real-time audio analysis, live sampling and spectral re-synthesis to build a vocabulary of possible performative responses to live instrumental input throughout an improvisatory performance. A form of timbral matching is employed to form a link between the live performer and an expanding database of musical materials. In addition, the system takes into account the unique nature of the rehearsal/practice space in musical performance through the implementation of performer-configurable cumulative rehearsal databases into the final design. This paper discusses the system in detail with reference to related work in the field, making specific reference to the system's interactive potential both inside and outside of a real-time performance context.},
	booktitle = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	publisher = {University of Michigan},
	author = {Carey, Benjamin},
	year = {2012},
	keywords = {improvisation, Interactivity, performance systems},
	file = {Full Text:/Users/Vincent/Zotero/storage/D4R2BDE5/Carey - 2012 - Designing for Cumulative Interactivity The _deriv.pdf:application/pdf}
}

@inproceedings{carey2013,
	address = {Sydney Australia},
	title = {\_derivations: {Improvisation} for {Tenor} {Saxophone} and {Interactive} {Performance} {System}},
	isbn = {978-1-4503-2150-1},
	shorttitle = {\_derivations},
	doi = {10.1145/2466627.2481226},
	language = {en},
	booktitle = {Proceedings of the 9th {ACM} {Conference} on {Creativity} \& {Cognition}},
	publisher = {ACM},
	author = {Carey, Benjamin},
	month = jun,
	year = {2013},
	pages = {411--412}
}

@book{carey2016,
	title = {\_derivations and the {Performer}-{Developer} : {Co}-{Evolving} {Digital} {Artefacts} and {Human}-{Machine} {Performance} {Practices}},
	shorttitle = {\_derivations and the {Performer}-{Developer}},
	abstract = {.................................................................................................................. ix Introduction .......................................................................................... 1 Chapter 1. 1.},
	language = {en},
	author = {Carey, Benjamin Leigh},
	year = {2016}
}

@article{ciufo2005,
	title = {Beginner's {Mind}: {An} {Environment} for {Sonic} {Improvisation}},
	abstract = {Beginner's Mind is the newest work in the ongoing Sonic Improvisation Series. This performance system is a combination of software and hardware designed for real-time sonic exploration. This paper will describe the primary design strategies employed in this system, and discuss key aesthetic concerns.},
	language = {en},
	author = {Ciufo, Thomas},
	year = {2005},
	pages = {4}
}

@book{davis2013,
	title = {Human-{Computer} {Co}-{Creativity}: {Blending} {Human} and {Computational} {Creativity}},
	shorttitle = {Human-{Computer} {Co}-{Creativity}},
	abstract = {This paper describes a thesis exploring how computer programs can collaborate as equals in the artistic creative process. The proposed system, CoCo Sketch, encodes some rudimentary stylistic rules of abstract sketching and music theory to contribute supplemental lines and music while the user sketches. We describe a three-part research method that includes defining rudimentary stylistic rules for abstract line drawing, exploring the interaction design for artistic improvisation with a computer, and evaluating how CoCo Sketch affects the artistic creative process. We report on the initial results of early investigations into artistic style that describe cognitive, perceptual, and behavioral processes used in abstract artists making.},
	language = {en},
	author = {Davis, N.},
	year = {2013}
}

@article{dudas2010,
	title = {Comprovisation: {The} {Various} {Facets} of {Composed} {Improvisation} within {Interactive} {Performance} {Systems}},
	shorttitle = {Comprovisation},
	doi = {10.1162/LMJ_a_00009},
	abstract = {ABSTRACT This article discusses the balance between composition and improvisation with respect to interactive performance using electronic and computer-based music systems. The author uses his own experience in this domain in the roles of both collaborator and composer as a point of reference to look at general trends in composed improvisation within the electronic and computer music community. Specifically, the intention is to uncover the limits and limitations of improvisation and its relationship to both composition and composed instruments within the world of interactive electronic musical performance.},
	journal = {Leonardo Music Journal},
	author = {Dudas, R.},
	year = {2010}
}

@article{eerola2018,
	title = {Shared {Periodic} {Performer} {Movements} {Coordinate} {Interactions} in {Duo} {Improvisations}},
	doi = {10.1098/rsos.171520},
	abstract = {Human interaction involves the exchange of temporally coordinated, multimodal cues. Our work focused on interaction in the visual domain, using music performance as a case for analysis due to its temporally diverse and hierarchical structures. We made use of two improvising duo datasets— (i) performances of a jazz standard with a regular pulse and (ii) non-pulsed, free improvizations— to investigate whether human judgements of moments of interaction between co-performers are influenced by body movement coordination at multiple timescales. Bouts of interaction in the performances were manually annotated by experts and the performers' movements were quantified using computer vision techniques. The annotated interaction bouts were then predicted using several quantitative movement and audio features. Over 80\% of the interaction bouts were successfully predicted by a broadband measure of the energy of the cross-wavelet transform of the co-performers' movements in non-pulsed duos. A more complex model, with multiple predictors that captured more specific, interacting features of the movements, was needed to explain a significant amount of variance in the pulsed duos. The methods developed here have key implications for future work on measuring visual coordination in musical ensemble performances, and can be easily adapted to other musical contexts, ensemble types and traditions.},
	journal = {Royal Society Open Science},
	author = {Eerola, T. and Jakubowski, K. and Moran, Nikki and Keller, P. and Clayton, M.},
	year = {2018},
	file = {Full Text:/Users/Vincent/Zotero/storage/2RW6WKD4/Eerola et al. - 2018 - Shared Periodic Performer Movements Coordinate Int.pdf:application/pdf}
}

@inproceedings{flores2019,
	address = {Porto Alegre, Brazil},
	title = {{HypeSax}: {Saxophone} {Acoustic} {Augmentation}},
	doi = {10.5281/zenodo.3672996},
	abstract = {New interfaces allow performers to access new possibilities of musical expression. Even though interfaces are often designed to be adaptable to different software, most of them rely on external speakers or similar transducers. This often results on disembodiment and acoustic disengagement from the interface, and in the case of augmented instruments, from the instruments themselves. This paper describes a project in which a hybrid system allows an acoustic integration between the sound of acoustic saxophone and electronics.},
	booktitle = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	publisher = {UFRGS},
	author = {Flores, Cristohper Ramos and Murphy, Jim and Norris, Michael},
	editor = {Queiroz, Marcelo and Sedó, Anna Xambó},
	month = jun,
	year = {2019},
	pages = {365--370},
	file = {Full Text:/Users/Vincent/Zotero/storage/F4PRFYSV/Flores et al. - 2019 - HypeSax Saxophone Acoustic Augmentation.pdf:application/pdf}
}

@article{herremans2017,
	title = {A {Functional} {Taxonomy} of {Music} {Generation} {Systems}},
	doi = {10.1145/3108242},
	abstract = {Digital advances have transformed the face of automatic music generation since its beginnings at the dawn of computing. Despite the many breakthroughs, issues such as the musical tasks targeted by different machines and the degree to which they succeed remain open questions. We present a functional taxonomy for music generation systems with reference to existing systems. The taxonomy organizes systems according to the purposes for which they were designed. It also reveals the inter-relatedness amongst the systems. This design-centered approach contrasts with predominant methods-based surveys and facilitates the identification of grand challenges to set the stage for new breakthroughs.},
	journal = {ACM Comput. Surv.},
	author = {Herremans, Dorien and Chuan, C. and Chew, Elaine},
	year = {2017},
	file = {Submitted Version:/Users/Vincent/Zotero/storage/4F5WIHS3/Herremans et al. - 2017 - A Functional Taxonomy of Music Generation Systems.pdf:application/pdf}
}

@inproceedings{hsu2006,
	address = {Paris, France},
	title = {Managing {Gesture} and {Timbre} for {Analysis} and {Instrument} {Control} in an {Interactive} {Environment}},
	doi = {10.5281/zenodo.1176927},
	abstract = {This paper describes recent enhancements in an interactive system designed to improvise with saxophonist John Butcher [1]. In addition to musical parameters such as pitch and loudness, our system is able to analyze timbral characteristics of the saxophone tone in real-time, and use timbral information to guide the generation of response material. We capture each saxophone gesture on the fly, extract a set of gestural and timbral contours, and store them in a repository. Improvising agents can consult the repository when generating responses. The gestural or timbral progression of a saxophone phrase can be remapped or transformed; this enables a variety of response material that also references audible contours of the original saxophone gestures. A single simple framework is used to manage gestural and timbral information extracted from analysis, and for expressive control of virtual instruments in a free improvisation context.},
	booktitle = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	author = {Hsu, William},
	year = {2006},
	keywords = {instrument control., Interactive music systems, timbre analysis},
	pages = {376--379},
	file = {Full Text:/Users/Vincent/Zotero/storage/7B9HDFJS/Hsu - 2006 - Managing Gesture and Timbre for Analysis and Instr.pdf:application/pdf}
}

@inproceedings{leeuw2009,
	address = {Pittsburgh, PA, United States},
	title = {The {Electrumpet} , a {Hybrid} {Electro}-{Acoustic} {Instrument}},
	doi = {10.5281/zenodo.1177613},
	abstract = {The Electrumpet is an enhancement of a normal trumpet with a variety of electronic sensors and buttons. It is a new hybrid instrument that facilitates simultaneous acoustic and electronic playing. The normal playing skills of a trumpet player apply to the new instrument. The placing of the buttons and sensors is not a hindrance to acoustic use of the instrument and they are conveniently located. The device can be easily attached to and detached from a normal Bb-trumpet. The device has a wireless connection with the computer through Bluetooth-serial (Arduino). Audio and data processing in the computer is effected by three separate instances of MAX/MSP connected through OSC (controller data) and Soundflower (sound data). The current prototype consists of 7 analogue sensors (4 valve-like potentiometers, 2 pressure sensors, 1 "Ribbon" controller) and 9 digital switches. An LCD screen that is controlled by a separate Arduino (mini) is attached to the trumpet and displays the current controller settings that are sent through a serial connection.},
	booktitle = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	author = {Leeuw, Hans},
	year = {2009},
	keywords = {Bluetooth, LCD, low latency, MAX/MSP., multiple Arduinos, OSC, Trumpet},
	pages = {193--198},
	file = {Full Text:/Users/Vincent/Zotero/storage/XTV7V3TB/Leeuw - 2009 - The Electrumpet , a Hybrid Electro-Acoustic Instru.pdf:application/pdf}
}

@book{lippe2002,
	title = {Real - {Time} {Interaction} {Among} {Composers}, {Performers}, and {Computer} {Systems}},
	abstract = {As the computer becomes more and more ubiquitous in society, the term “interactive” has become a widely used, but misunderstood term. In this paper, I discuss definitions of interactive music and interactive music systems, performance issues in interactive music, and performer/machine relationships that engender interaction in an attempt to explain how and why I pursue this discipline. Furthermore, I describe the function of computers in my compositions, and the manner in which I explore performer/machine interaction. Personal Background Prior to 1980, I was equally active as both an instrumental and electronic music composer. I was originally drawn to the computer because of a strong interest in designing new sounds, something at which computers are quite good, and exploring algorithmic compositional structures, simulation being another particular strength of computers. While new sounds and compositional algorithms can be readily explored in a non-real-time environment, and the tape and instrument paradigm has existed since the beginnings of electronic music, my first experiences with electronic music were with analog synthesizers, and my earliest experiences with computers in the 1970s were predominately with real-time systems. In addition, during the second half of the 1970s, I worked with an improvisational music/dance ensemble exploring live-electronics combined with acoustic instruments. For the past 20 years, I have pursued creative and research interests in interactive computer music involving live instrumentalists and computers in performance situations. The opportunity to combine instruments and computers became increasing practical as real-time digital signal processors became more available in the 1980s. As the level of refinement of real-time control became proportionally greater, it became evident that composed music for instruments and interactive computer systems was a viable medium of expression. And while it is true that interactive computer music is a relatively new area in the electronic music field, developments in the power of desktop computers and the sophistication of real-time software have been responsible for enormous growth of the genre in the last ten years. The challenge of interactive computer music has been to articulate sonic design and compositional structures in some sort of interactive relationship with live performers. While some composers use computers to model or imitate musical instruments, and others are interested in modeling human performance, I am not interested in replacing either instruments or performers. Musicians, with their years of experience playing on instruments which have often developed over centuries, offer rich musical and cultural potential, and are perhaps the ultimate driving force for me as a composer working with computer technology. Interactive Music Robert Rowe, in the seminal book Interactive Music Systems [Rowe, 1993], states: “Interactive music systems are those whose behavior changes in response to musical input”. A dictionary definition of the word interactive states: “capable of acting on or influencing each other”. This would imply that a feedback loop of some sort exists between performer and machine. Indeed, the Australian composer Barry Moon suggests: “levels of interaction can be gauged by the potential for change in the behaviors of computer and performer in their response to each other”. [Moon, 1997]. George Lewis, a pioneer in the field of interactive computer music, has stated that much of what passes for interactive music today is in reality just simple event “triggering”, which does not involve interaction except on the most primitive level. He also states that since (Euro-centric) composers often strive for control over musical structure and sound, this leads many composers to confuse triggering with real interaction. He describes an interactive system as “one in which the structures present as inputs are processed in quite a complex, multi-directional fashion. Often the output behavior is not immediately traceable to any particular input event.” [Rowe et al, 1992-93]. David Rokeby, the Toronto-based interactive artist, states that interaction transcends control, and in a successful interactive environment, direct correspondences between actions and results are not perceivable. In other words, if performers feel they are in control of (or are capable of controlling) an environment, then they cannot be truly interacting, since control is not interaction. [Rokeby, 1997]. Clearly, on a continuum from triggering to Rokeby's non-control there is a great deal of latitude to loosely label a wide range of artistic approaches and human/machine relationships as “interactive”. Fortunately, a composer can assign a variety of roles to a computer in an interactive music environment. The computer can be given the role of instrument, performer, conductor, and/or composer. These roles can exist simultaneously and/or change continually, and it is not necessary to conceive of this continuum horizontally. On a vertical axis everything from simple triggering to Rokeby-like interaction can have the potential to exist simultaneously. If performer and machine are to be equal in an interactive environment, one could argue that the performer should also be offered a similar variety of roles. A performer is already a performer, and already plays an instrument; therefore, a composer can assign the role of conductor and composer to a performer. The conductor role comes quite naturally to a performer in an interactive environment, and is commonly exploited. The composer role is more problematic: composers exploring algorithmic processes are often willing to allow aspects of a composition to be decided via a computer program, as this is inherent in the very nature of certain approaches to algorithmic composition. But some composers question the idea of allowing a performer to take on the role of composer, and usually this involves giving the performer some degree of freedom to improvise. While I am not attempting to equate performer improvisation with algorithmic procedures, there are certainly interesting connections between the two which are outside the scope of this paper. But if we look closely at the various explanations of interactive music, most of them imply or clearly involve some shifting of the composer's “responsibilities” towards the computer system, towards the performer, or both. Interaction is a very complex subject, and we are probably only at the beginnings of a discussion of human/machine relationships, which I suspect will continue to develop over many years. Musical Interactivity While this discussion about interaction is fascinating on aesthetic, philosophical, and humanistic levels, at the practical level of music-making the quantity or quality of human/machine interactivity that takes place is much less important to me than the quality of musical interactivity. Musical interactivity is something that happens when people play music together. Rich musical interactivity exists in music that has no computer or electronic part, and can even be found in instrument/tape pieces (albeit only the musician is actively interacting), so the level of interactivity of a computer system is really a secondary consideration from a musical point of view. While I stated that I am not interested in modeling performers or instruments, I am interested in using the computer to model the relationship that exists between musicians during a performance. This relationship involves a subtle kind of listening while playing in a constantly changing dialogue among players, often centering on expressive timing and/or expressive use of dynamics and articulation. We often refer to this aspect of music as “phrasing”, an elusive, yet highly important part of performance. While concepts of musical interpretation exist in solo performance, and one can discuss a solo performer's “interaction” with a score, the musical interactivity that exists when two or more musicians play together is a more appropriate model for describing interaction between musicians and computers in real-time. Musical Expression in Performance At its core, European music notation has developed as a Cartesian coordinate system in which precise and measurable information about just two parameters, frequency and time, are specified. In a typical performance we assume that frequency will be respected, usually rather precisely, while we assume that time will be respected in a less precise manner. But, within certain boundaries, variants in both frequency and time are expected and considered part of an expressive performance. Vibrato, portamento, rubato, accelerando, ritardando, etc., are all common markings used by composers and are part of the interpretive toolbox which a performer is expected to exploit in transforming frequency and time. Some composers prefer not take responsibility for notating subtle variations in frequency and time beyond the notated pitches and rhythms, so that expressive decisions are left to the performers' discretion. Other composers, who perhaps prefer to rely less on cultural conventions, specify variations of pitch and time in greater detail. Beyond frequency and time notation there exists an enormous variety of imprecise notation that can be found in any glossary of musical terms: staccato, legato, mezzo-forte, crescendo, sul ponticello, etc. Since these notations are not as easily measurable as pitch and time, they are considered more in the domain of performers. More importantly, a performance is judged, not by whether the pitches and rhythms are correct (this is a given), but by how well the performer expressively alters pitches to a small degree, and rhythm to a larger extent, while interpreting expressive markings pertaining to parameters of loudness, timbre, articulation, etc., with a rather large degree of freedom. The interpretation of these parameters is subjective a},
	language = {en},
	author = {Lippe, C.},
	year = {2002}
}

@phdthesis{masone216,
	type = {{PhD} {Thesis}},
	title = {The {Contemporary} {Bassoonist}: {Music} for {Interactive} {Electroacoustics} and {Bassoon}},
	abstract = {As the bassoon has evolved over time, the music written for the instrument has evolved around it, and was many times the catalyst for its evolution. Bassoon music of the seventeenth through early twentieth centuries has defined much of the curricula for bassoon studies, and has established how we consider and experience the bassoon. We experience, write, and consume music in vastly different ways than just a generation ago. Humans use technology for the most basic of tasks. Composers are using the technology of our generation to compose music that is a reflection of our time. This is a significant aspect of art music today, and bassoonists are barely participating in the creation of this new repertoire. Performance practice often considers only the musical score; interactive electronic music regularly goes beyond that. The combination of technological challenges and inexperience can make approaching electroacoustic music a daunting and inaccessible type of music for bassoonists. These issues require a different language to the performance practice: one that addresses music, amplification, computer software, hardware, the collaboration between performer and technology, and often the performer and composer. The author discusses problems that performers face when rehearsing and performing interactive electroacoustic works for bassoon, and offers some solutions.},
	language = {en},
	author = {Masone, Jolene Karen},
	year = {216}
}

@book{nika2015,
	title = {Guided {Improvisation} as {Dynamic} {Calls} to an {Offline} {Model}},
	abstract = {This paper describes a reactive architecture handling the hybrid temporality of guided human-computer music improvisation. It aims at combining reactivity and anticipation in the music generation processes steered by a \&quot; scenario \&quot;. The machine improvisation takes advantage of the temporal structure of this scenario to generate short-term anticipations ahead of the performance time, and reacts to external controls by refining or rewriting these anticipa-tions over time. To achieve this in the framework of an interactive software, guided improvisation is modeled as embedding a compositional process into a reactive architecture. This architecture is instantiated in the improvisation system ImproteK and implemented in OpenMusic.},
	language = {en},
	author = {Nika, Jérôme and Bouche, Dimitri and Bresson, Jean and Chemillier, M. and Assayag, G.},
	year = {2015}
}

@inproceedings{nika2016,
	title = {Guiding {Human}-{Computer} {Music} {Improvisation}: {Introducing} {Authoring} and {Control} with {Temporal} {Scenarios}. ({Guider} l'improvisation {Musicale} {Homme}-{Machine} : {Introduire} {Du} {Contrôle} et de {La} {Composition} {Avec} {Des} {Scénarios} {Temporels})},
	shorttitle = {Guiding {Human}-{Computer} {Music} {Improvisation}},
	abstract = {This thesis focuses on the introduction of authoring and controls in human-computer music improvisation through the use of temporal scenarios to guide or compose interactive performances, and addresses the dialectic between planning and reactivity in interactive music systems dedicated to improvisation. An interactive system dedicated to music improvisation generates music “on the fly”, in relation to the musical context of a live performance. This work follows on researches on machine improvisation seen as the navigation through a musical memory: typically the music played by an “analog” musician co-improvising with the system during a performance or an offline corpus. These researches were mainly dedicated to free improvisation, and we focus here on pulsed and “idiomatic” music. Within an idiomatic context, an improviser deals with issues of acceptability regarding the stylistic norms and aesthetic values implicitly carried by the musical idiom. This is also the case for an interactive music system that would like to play jazz, blues, or rock... without being limited to imperative rules that would not allow any kind of transgression or digression. Various repertoires of improvised music rely on a formalized and temporally structured object, for example a harmonic progression in jazz improvisation. The same way, the models and architecture we developed rely on a formal temporal structure. This structure does not carry the narrative dimension of the improvisation, that is its fundamentally aesthetic and non-explicit evolution, but is a sequence of formalized constraints for the machine improvisation. This thesis thus presents: a music generation model guided by a “scenario” introducing mechanisms of anticipation; a framework to compose improvised interactive performances at the “scenario” level; an architecture combining anticipatory behavior with reactivity using mixed static/dynamic scheduling techniques; an audio rendering module to perform live re-injection of captured material in synchrony with a non-metronomic beat; a study carried out with ten musicians through performances, work sessions, listening sessions and interviews. First, we propose a music generation model guided by a formal structure. In this framework “improvising” means navigating through an indexed memory to collect some contiguous or disconnected sequences matching the successive parts of a “scenario” guiding the improvisation (for example a chord progression). The musical purpose of the scenario is to ensure the conformity of the improvisations generated by the machine to the idiom it carries, and to introduce anticipation mechanisms in the generation process, by analogy with a musician anticipating the resolution of a harmonic progression. Using the formal genericity of the couple “scenario / memory”, we sketch a protocol to compose improvisation sessions at the scenario level. Defining scenarios described using audio-musical descriptors or any user-defined alphabet can lead to approach others dimensions of guided interactive improvisation. In this framework, musicians for whom the definition of a musical alphabet and the design of scenarios for improvisation is part of the creative process can be involved upstream, in the “meta-level of composition” consisting in the design of the musical language of the machine. This model can be used in a compositional workflow and is “offline” in the sense that one run produces a whole timed and structured musical gesture satisfying the designed scenario that will then be unfolded through time during performance. We present then a dynamic architecture embedding such generation processes with formal specifications in order to combine anticipation and reactivity in a context of guided improvisation. In this context, a reaction of the system to the external environment, such as control interfaces or live players input, cannot only be seen as a spontaneous instant response. Indeed, it has to take advantage of the knowledge of this temporal structure to benefit from anticipatory behavior. A reaction can be considered as a revision of mid-term anticipations, musical sequences previously generated by the system ahead of the time of the performance, in the light of new events or controls. To cope with the issue of combining long-term planning and reactivity, we therefore propose to model guided improvisation as dynamic calls to “compositional” processes, that it to say to embed intrinsically offline generation models in a reactive architecture. In order to be able to play with the musicians, and with the sound of the musicians, this architecture includes a novel audio rendering module that enables to improvise by re-injecting live audio material (processed and transformed online to match the scenario) in synchrony with a non-metronomic fluctuating pulse. Finally, this work fully integrated the results of frequent interactions with expert musicians to the iterative design of the models and architectures. These latter are implemented in the interactive music system ImproteK, one of the offspring of the OMax system, that was used at various occasions during live performances with improvisers. During these collaborations, work sessions were associated to listening sessions and interviews to gather the evaluations of the musicians on the system in order to validate and refine the scientific and technological choices.},
	author = {Nika, Jérôme},
	year = {2016}
}

@inproceedings{nika2017,
	address = {Shangai, China},
	title = {{DYCI2} {Agents}: {Merging} the ”free”, ”reactive”, and ”scenario-{Based}” {Music} {Generation} {Paradigms}},
	shorttitle = {{DYCI2} {Agents}},
	abstract = {The collaborative research and development project DYCI2, Creative Dynamics of Improvised Interaction, focuses on conceiving, adapting, and bringing into play efficient models of artificial listening, learning, interaction, and generation of musical contents. It aims at developing creative and autonomous digital musical agents able to take part in various human projects in an interactive and artistically credible way; and, in the end, at contributing to the perceptive and communicational skills of embedded artificial intelligence. The concerned areas are live performance, production, pedagogy, and active listening. This paper gives an overview focusing on one of the three main research issues of this project: conceiving multi-agent architectures and models of knowledge and decision in order to explore scenarios of music co-improvisation involving human and digital agents. The objective is to merge the usually exclusive "free" , "reactive", and "scenario-based" paradigms in interactive music generation to adapt to a wide range of musical contexts involving hybrid temporality and multimodal interactions.},
	booktitle = {International {Computer} {Music} {Conference}},
	author = {Nika, Jérôme and Déguernel, Ken and Chemla–Romeu-Santos, Axel and Vincent, Emmanuel and Assayag, Gérard},
	month = oct,
	year = {2017}
}

@inproceedings{pardo2016,
	title = {Expansion of {Musical} {Styles}, {Function} of {Texture}, and {Performing} {Techniques} in {Brian} {Lock}'s {Sonic} {Archaeologies} {No}. 1},
	abstract = {British composer Brian Lock merges the composition styles of Alexander Goehr, Henryk Gorecki and Witold Lutoslawski in his innovative works for instrumental sounds and electronics. His most recent work for flute, Sonic Archaeologies No.1, was premiered at the University of North Texas by Mary Karen Clardy, flute; Brian Lock, piano/electric keyboard; and Daniel Pardo, laptop/live mixing. The purpose of this dissertation is to provide flutists with artistic and technical guidance in preparing this work for flute, prerecorded orchestra, interactive electronics and improvisatory accompaniment. Sonic Archaeologies No. 1, a piece in five movements (Black Rain, Psychomania, Kodo, Susperia, and Deep in the Machine), incorporates contemporary techniques to create sounds other than the Western concert flute, with the use of live reinforcement devices such as microphones and time-based audio effects within a D.A.W. (Digital Audio Workstation.) Reggae, Hip-Hop and cinematic styles are juxtaposed within the work, fusing current genres with traditional rhythmic forms like the ones found in a bourree. As the solo instrument, flute provides more textural than melodic elements, and the performer is required to interact with an unpredictable sonic soundscape as a result of the improvisatory element of the keyboards and computer. The notation of Sonic Archaeologies No.1 invites interpretation blending and altering traditional sounds through microphones and a processed signal flow. The performance guide will address acoustical considerations when the flute sound is being manipulated by dynamic and time-based processors in live performance; the interaction between the flute, electronics and acoustic spaces; the elements of sound production that provide interpretation of contemporary popular styles; and the opportunities for the performer to find, explore and develop artistry beyond the limitations of music notation.},
	author = {Pardo, Daniel},
	year = {2016}
}

@article{pasquier2017,
	title = {An {Introduction} to {Musical} {Metacreation}},
	volume = {14},
	doi = {10.1145/2930672},
	abstract = {Musical metacreation (MuMe), also known as musical computational creativity, is a subfield of computational creativity that focuses on endowing machines with the ability to achieve creative musical tasks, such as composition, interpretation, improvisation, accompaniment, mixing, etc. It covers all dimensions of the theory and practice of computational generative music systems, ranging from purely artistic approaches to purely scientific ones, inclusive of discourses relevant to this topic from the humanities. MuMe systems range from purely generative ones to a variety of interactive systems, such as those for computer-assisted composition and computer-assisted sound design. In order to better appreciate the many dimensions of this interdisciplinary domain and see how it overlaps and differs from research in computer music, this introduction provides a general entry point. After defining and introducing the domain, its context, and some of its terminology, we reflect on some challenges and opportunities for the field as a whole.},
	number = {2},
	journal = {Computers in Entertainment},
	author = {Pasquier, Philippe and Eigenfeldt, Arne and Bown, Oliver and Dubnov, Shlomo},
	year = {2017},
	pages = {2:1--2:14}
}

@book{penny2009,
	title = {The {Extended} {Flautist}: {Techniques}, {Technologies} and {Performer} {Perceptions} in {Music} for {Flute} and {Electronics}},
	shorttitle = {The {Extended} {Flautist}},
	abstract = {............................................................................................................................. ii LIST OF FIGURES................................................................................................................viii LIST OF TABLES .................................................................................................................... x LIST OF MIND MAPS ........................................................................................................... xi ACKNOWLEDGEMENTS.................................................................................................... xii PRELUDE ..................................................................................................................................... 1 PART I: THE PROJECT.......................................................................................2},
	language = {en},
	author = {Penny, M. J.},
	year = {2009}
}

@inproceedings{puckette1993,
	title = {Nonobvious {Roles} for {Electronics} in {Performance} {Enhancement}},
	abstract = {A new paradigm is proposed which overcomes certain shortcomings in our current models of performer/computer interaction. The "Standard Model", in which the computer passively waits for performer input can be extended to include closed-loop behavior, both stable and unstable. In the time range typiied by the muscular and auditory subsystems, the existence Cadoz] of approximately linear spatially modelled feedback is emphasized. On a longer time frame, the possibilities are more subtile and powerful. In a well-designed timbre space / control space Wessel] the connection occurs on a more musical level and is evidenced in enhancements in the performer's own apparent output.},
	booktitle = {{ICMC}},
	author = {Puckette, M. and Settel, Zack},
	year = {1993}
}

@book{rocha2009,
	title = {{THE} {HYPER}-{KALIMBA}: {DEVELOPING} {AN} {AUGMENTED} {INSTRUMENT} {FROM} {A} {PERFORMER}'{S} {PERSPECTIVE}},
	shorttitle = {{THE} {HYPER}-{KALIMBA}},
	abstract = {The paper describes the development of the hyper-kalimba, an augmented instrument created by the authors. This development was divided into several phases and was based on constant consideration of technology, performance and compositionalissues. Thebasicgoalwastoextendthesound possibilities of the kalimba, without interfering with any of the original features of the instrument or with the performer's pre-existing skills. In this way performers were able to use all the traditional techniques previously developed, while learning and exploring all the new possibilities added to the instrument.},
	language = {en},
	author = {Rocha, F. and Escolade, M. and Malloch, Joseph W.},
	year = {2009}
}

@article{rowe1999,
	title = {The {Aesthetics} of {Interactive} {Music} {Systems}},
	doi = {10.1080/07494469900640361},
	abstract = {Computers in music have made possible new kinds of composition at the same time that they have caused upheaval in the social and cultural practice of music making. Interactive music systems have a particular place in this context in that they explore some highly specific techniques of composition at the same time that they create a novel and engaging form of interaction between humans and computers. In this essay, real-time algorithmic composition in works including improvisation are considered as well as the contrasts between interactive and tape music. The author's composition Maritime for violin and interactive music system is presented as an illustration of the aesthetic viewpoint developed.},
	author = {Rowe, R.},
	year = {1999},
	file = {Full Text:/Users/Vincent/Zotero/storage/JHIFEPLP/Rowe - 1999 - The Aesthetics of Interactive Music Systems.pdf:application/pdf}
}

@inproceedings{summers2018,
	title = {Augmenting an {Improvised} {Practice} on the {Viola} {Da} {Gamba}},
	abstract = {This thesis examines my improvisatory practice on the viola da gamba and its augmentation with mixed-music computer systems. It comprises creative work and an extended written commentary and discussion. My creative work is presented in two albums of music – solo viola da gamba improvisation, and viola da gamba and mixed-music computer systems – and supplementary recorded material. The written commentary looks in depth at the presented creative work. I use the first, solo album to examine my improvisatory practice. To explore augmenting this practice with systems, I look in detail at my performances with gruntCount by Martin Parker, Laminate by myself and derivations by Ben Carey. Examples of these performances are presented in the second album. Scrutiny of these three systems leads to extended discussion of the following topics: 1. Taxonomy: What are these systems? What are the characteristics they display? Do these systems fit into a standard classification scheme? 2. Ontology: Do performances with these systems instantiate musical works? What are the criteria that would help us to decide? How much of my practice is therefore underpinned by musical works? 3. Copyright: Who is responsible for the musical output with these systems? Who is a legal/musical author in such performances? To conclude, I compare my improvisatory practice with and without systems and identify learnings arising from this research.},
	author = {Summers, M.},
	year = {2018}
}

@article{welch2010,
	title = {Programming {Machines} and {People}: {Techniques} for {Live} {Improvisation} with {Electronics}},
	shorttitle = {Programming {Machines} and {People}},
	doi = {10.1162/LMJ_a_00008},
	abstract = {ABSTRACT Many performers of new music do not come from an improvising tradition, and the addition of live electronics to works written for these performers may be intimidating due to their inexperience with improvising and/or working with technology. Although inexperience may be a problem, it can be overcome. The author describes techniques and strategies for creating rule-based improvisation environments with live electronics.},
	journal = {Leonardo Music Journal},
	author = {Welch, Chapman},
	year = {2010}
}

@article{kimura2003,
	title = {Creative process and performance practice of interactive computer music: a performer's tale},
	volume = {8},
	issn = {1355-7718, 1469-8153},
	shorttitle = {Creative process and performance practice of interactive computer music},
	url = {https://www.cambridge.org/core/product/identifier/S1355771803000268/type/journal_article},
	doi = {10.1017/S1355771803000268},
	abstract = {I have had a major interest in the performance practice issues in electronic and interactive systems over the years (see, for example, Kimura 1996). As a performer/composer often presenting pieces from the classical and other contemporary acoustic violin literature in traditional settings along with electronic works, and also as a teacher of interactive computer music performance at a conservatory where my students include highly trained performers, performance practice issues in computer music come up very frequently in association with the creative process. I tend to focus on creating MaxMSP patches that address a particular musical context or situation, rather than creating an elaborate versatile and reusable MaxMSP patch and then using that patch in a particular way to make music. This paper describes a few examples of my interest in this area: (i) System Aspects: Performance Practice Issues and Room Acoustics; (ii) ‘Pragmatic’ Programming and Performance of Interactive Music; and (iii) Creative Process and Interactive Computer Music.},
	language = {en},
	number = {3},
	urldate = {2021-02-05},
	journal = {Organised Sound},
	author = {Kimura, Mari},
	month = dec,
	year = {2003},
	pages = {289--296}
}

@inproceedings{assayag2006,
	address = {Santa Barbara, United States},
	title = {{OMAX} {Brothers}: {A} {Dynamic} {Topology} of {Agents} for {Improvisation} {Learning}},
	shorttitle = {{OMAX} {Brothers}},
	url = {https://hal.inria.fr/hal-00839075},
	abstract = {no abstract},
	urldate = {2021-04-05},
	booktitle = {{ACM} {Multimedia} {Workshop} on {Audio} and {Music} {Computing} for {Multimedia}},
	publisher = {Santa Barbara},
	author = {Assayag, Gerard and Bloch, George and Chemillier, Marc and Cont, Arshia and Dubnov, Shlomo},
	year = {2006},
	file = {HAL PDF Full Text:/Users/Vincent/Zotero/storage/TB2DXWUP/Assayag et al. - 2006 - OMAX Brothers A Dynamic Topology of Agents for Im.pdf:application/pdf}
}

@article{nika2017a,
	title = {{ImproteK}: introducing scenarios into human-computer music improvisation},
	shorttitle = {{ImproteK}},
	url = {https://hal.archives-ouvertes.fr/hal-01380163},
	doi = {10.1145/3022635},
	abstract = {This article focuses on the introduction of control, authoring, and composition in human-computer music improvisation through the description of a guided music generation model and a reactive architecture, both implemented in the software ImproteK. This interactive music system is used with expert improvisers in work sessions and performances of idiomatic and pulsed music, and more broadly in situations of structured or composed improvisation. The article deals with the integration of specifications in the music generation process by means of a fixed or dynamic "scenario", and addresses the issue of the dialectic between reactivity and planning in interactive music improvisation. It covers the different levels involved in the machine improvisation: the integration of anticipation relative to a predefined structure in a guided generation process at a symbolic level, an architecture combining this anticipation with reactivity using mixed static/dynamic scheduling techniques, and an audio rendering module performing live re-injection of captured material in synchrony with a non-metronomic beat. Finally, it sketches a framework to compose improvisation sessions at the scenario level, extending the initial musical scope of the system. All these points are illustrated by videos of performances or work sessions with musicians.},
	urldate = {2021-04-05},
	journal = {ACM Computers in Entertainment},
	author = {Nika, Jérôme and Chemillier, Marc and Assayag, Gérard},
	month = jan,
	year = {2017},
	keywords = {Abstraction, Combinatorics on words, Heuristic function construction, Intelligent agents, Markov processes, modeling and modularity, Modeling methodologies, Motif discovery, Music retrieval, Performing arts, Planning under uncertainty, Real-time system architecture, Sound and music computing, Symbolic and algebraic algorithms},
	file = {HAL PDF Full Text:/Users/Vincent/Zotero/storage/QZZY8PXL/Nika et al. - 2017 - ImproteK introducing scenarios into human-compute.pdf:application/pdf}
}

@inproceedings{fornari2017,
	title = {An {Algorithm} for {Guiding} {Expectation} in {Free} {Improvisational} {Solo} {Performances}},
	abstract = {Free improvisation lets a performer to openly explore musical outcomes unbounded by any structure or notation. However, the human mind is naturally constrained by its own built-in habits. As such, musicians usually develop, during years of practice and aesthetic predilections, a repertoire of known musical patterns which are intentionally or even unconsciously used by them during musical improvisation. 
This work presents an algorithm that aims to retrieve in real-time similarities during sessions of free improvisations, This is done in order to inform the musician during performance which ones might be the most expected musical outcomes by the listeners.},
	author = {Fornari, Jose and Schaub, Stéphan},
	month = nov,
	year = {2017},
	file = {Full Text PDF:/Users/Vincent/Zotero/storage/X98CDFG3/Fornari and Schaub - 2017 - An Algorithm for Guiding Expectation in Free Impro.pdf:application/pdf}
}

@article{hoffding2021,
	title = {Interactive expertise in solo and joint musical performance},
	volume = {198},
	issn = {1573-0964},
	url = {https://doi.org/10.1007/s11229-019-02339-x},
	doi = {10.1007/s11229-019-02339-x},
	abstract = {The paper presents two empirical cases of expert musicians—a classical string quartet and a solo, free improvisation saxophonist—to analyze the explanatory power and reach of theories in the field of expertise studies and joint action. We argue that neither the positions stressing top-down capacities of prediction, planning or perspective-taking, nor those emphasizing bottom-up embodied processes of entrainment, motor-responses and emotional sharing can do justice to the empirical material. We then turn to hybrid theories in the expertise debate and interactionist accounts of cognition. Attempting to strengthen and extend them, we offer ‘Arch’: an overarching conception of musical interaction as an externalized, cognitive scaffold that encompasses high and low-level cognition, internal and external processes, as well as the shared normative space including the musical materials in which the musicians perform. In other words, ‘Arch’ proposes interaction as a multivariate multimodal overarching scaffold necessary to explain not only cases of joint performance, but equally of solo improvisation.},
	language = {en},
	number = {1},
	urldate = {2021-04-09},
	journal = {Synthese},
	author = {Høffding, Simon and Satne, Glenda},
	month = jan,
	year = {2021},
	pages = {427--445},
	file = {Submitted Version:/Users/Vincent/Zotero/storage/XQ68JXJ5/Høffding and Satne - 2021 - Interactive expertise in solo and joint musical pe.pdf:application/pdf}
}

@inproceedings{fujii2012,
	title = {Autonomously {Acquiring} a {Video} {Game} {Agent}’s {Behavior}: {Letting} {Players} {Feel} {Like} {Playing} with a {Human} {Player}},
	volume = {7624},
	isbn = {978-3-642-34291-2},
	shorttitle = {Autonomously {Acquiring} a {Video} {Game} {Agent}’s {Behavior}},
	doi = {10.1007/978-3-642-34292-9_42},
	abstract = {Designing behavior patterns of video game agents (COM players) is a crucial aspect of video game development. While various systems aiming to automatically acquire behavior patterns has been proposed and some have successfully obtained stronger patterns than human players, the obtained behavior patterns looks mechanical. We present herein an autonomous acquisition of video game agent behavior, which emulates the behavior of a human player. Instead of implementing straightforward heuristics, the behavior is acquired using Q-learning, a reinforcement learning, where, biological constraints are imposed. In the experiments using Infinite Mario Bros., we observe that behaviors that imply human behaviors are obtained by imposing sensory error, perceptual and motion delay, and fatigue as biological constraints.},
	author = {Fujii, Nobuto and Sato, Yuichi and Wakama, Hironori and Katayose, Haruhiro},
	month = nov,
	year = {2012},
	pages = {490--493}
}

@inproceedings{mccormack2019,
	title = {In a {Silent} {Way}: {Communication} {Between} {AI} and {Improvising} {Musicians} {Beyond} {Sound}},
	shorttitle = {In a {Silent} {Way}},
	doi = {10.1145/3290605.3300268},
	abstract = {Collaboration is built on trust, and establishing trust with a creative Artificial Intelligence is difficult when the decision process or internal state driving its behaviour isn't exposed. When human musicians improvise together, a number of extra-musical cues are used to augment musical communication and expose mental or emotional states which affect musical decisions and the effectiveness of the collaboration. We developed a collaborative improvising AI drummer that communicates its confidence through an emoticon-based visualisation. The AI was trained on musical performance data, as well as real-time skin conductance, of musicians improvising with professional drummers, exposing both musical and extra-musical cues to inform its generative process. Uni- and bi-directional extra-musical communication with real and false values were tested by experienced improvising musicians. Each condition was evaluated using the FSS-2 questionnaire, as a proxy for musical engagement. The results show a positive correlation between extra-musical communication of machine internal state and human musical engagement.},
	author = {McCormack, Jon and Gifford, Toby and Hutchings, Patrick and Llano, Maria and Yee-King, Matthew and dʼInverno, Mark},
	month = apr,
	year = {2019},
	pages = {1--11},
	file = {Full Text PDF:/Users/Vincent/Zotero/storage/DBS4D5VK/McCormack et al. - 2019 - In a Silent Way Communication Between AI and Impr.pdf:application/pdf}
}

@book{pachet,
	title = {Chapter {N}+1: {Enhancing} {Individual} {Creativity} with {Interactive} {Musical} {Reflective} {Systems}},
	shorttitle = {Chapter {N}+1},
	author = {Pachet, François},
	file = {Citeseer - Full Text PDF:/Users/Vincent/Zotero/storage/JUYZVHSP/Pachet - Chapter N+1 Enhancing Individual Creativity with .pdf:application/pdf;Citeseer - Snapshot:/Users/Vincent/Zotero/storage/7LI8FBD6/download.html:text/html}
}

@article{gifford2018,
	title = {Computational systems for music improvisation},
	volume = {29},
	issn = {1462-6268},
	url = {https://doi.org/10.1080/14626268.2018.1426613},
	doi = {10.1080/14626268.2018.1426613},
	abstract = {Computational music systems that afford improvised creative interaction in real time are often designed for a specific improviser and performance style. As such the field is diverse, fragmented and lacks a coherent framework. Through analysis of examples in the field, we identify key areas of concern in the design of new systems, which we use as categories in the construction of a taxonomy. From our broad overview of the field, we select significant examples to analyse in greater depth. This analysis serves to derive principles that may aid designers scaffold their work on existing innovation. We explore successful evaluation techniques from other fields and describe how they may be applied to iterative design processes for improvisational systems. We hope that by developing a more coherent design and evaluation process, we can support the next generation of improvisational music systems.},
	number = {1},
	urldate = {2021-04-09},
	journal = {Digital Creativity},
	author = {Gifford, Toby and Knotts, Shelly and McCormack, Jon and Kalonaris, Stefano and Yee-King, Matthew and d’Inverno, Mark},
	month = jan,
	year = {2018},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/14626268.2018.1426613},
	keywords = {computational creativity, creative agency, evaluation, generative music, Improvisational interfaces},
	pages = {19--36},
	file = {Snapshot:/Users/Vincent/Zotero/storage/8HIYA4D3/14626268.2018.html:text/html;Accepted Version:/Users/Vincent/Zotero/storage/4IIE4GBG/Gifford et al. - 2018 - Computational systems for music improvisation.pdf:application/pdf}
}

@inproceedings{mccormack2016a,
	title = {Designing {Improvisational} {Interfaces}},
	url = {https://research.monash.edu/en/publications/designing-improvisational-interfaces},
	language = {English},
	urldate = {2021-04-09},
	booktitle = {Proceedings of the {Seventh} {International} {Conference} on {Computational} {Creativity}},
	publisher = {Sony CSL},
	author = {McCormack, Jon and d'Inverno, Mark},
	month = jul,
	year = {2016},
	pages = {98--105},
	file = {Snapshot:/Users/Vincent/Zotero/storage/V7QZZXV5/designing-improvisational-interfaces.html:text/html}
}

@article{mccormack2016,
	title = {Designing {Improvisational} {Interfaces}},
	abstract = {This paper examines the possibilities for creative interaction with computers, in particular modes of interaction based on improvisation and spontaneous creative discovery. We consider research ﬁndings from studies in psychology that investigate how humans improvise together to see what could be useful in helping us to design systems that provide new kinds of interactive opportunities. We draw on our personal experiences both as computer scientists working across art and music, and as practicing artists and musicians, to examine what artists and musicians would want in any system designed to support creative interaction with a computer. We bring together these different ﬁndings to propose a series of working principles which form a basis for designing systems that facilitate collaboration and improvisation with computers in creative domains.},
	language = {en},
	author = {McCormack, Jon},
	year = {2016},
	pages = {8},
	file = {McCormack - 2016 - Designing Improvisational Interfaces.pdf:/Users/Vincent/Zotero/storage/CE5MLJ7K/McCormack - 2016 - Designing Improvisational Interfaces.pdf:application/pdf}
}

@incollection{blackwell2012,
	address = {Berlin, Heidelberg},
	title = {Live {Algorithms}: {Towards} {Autonomous} {Computer} {Improvisers}},
	isbn = {978-3-642-31726-2 978-3-642-31727-9},
	shorttitle = {Live {Algorithms}},
	url = {http://link.springer.com/10.1007/978-3-642-31727-9_6},
	abstract = {A Live Algorithm is an autonomous machine that interacts with musicians in an improvised setting. This chapter outlines perspectives on Live Algorithm research, offering a high level view for the general reader, as well as more detailed and specialist analysis. The study of Live Algorithms is multi-disciplinary in nature, requiring insights from (at least) Music Technology, Artiﬁcial Intelligence, Cognitive Science, Musicology and Performance Studies. Some of the most important issues from these ﬁelds are considered. A modular decomposition and an associated set of wiring diagrams is offered as a practical and conceptual tool. Technical, behavioural, social and cultural contexts are considered, and some signposts for future Live Algorithm research are suggested.},
	language = {en},
	urldate = {2021-04-09},
	booktitle = {Computers and {Creativity}},
	publisher = {Springer Berlin Heidelberg},
	author = {Blackwell, Tim and Bown, Oliver and Young, Michael},
	editor = {McCormack, Jon and d’Inverno, Mark},
	year = {2012},
	doi = {10.1007/978-3-642-31727-9_6},
	pages = {147--174},
	file = {Blackwell et al. - 2012 - Live Algorithms Towards Autonomous Computer Impro.pdf:/Users/Vincent/Zotero/storage/KQ9N7AMW/Blackwell et al. - 2012 - Live Algorithms Towards Autonomous Computer Impro.pdf:application/pdf}
}

@inproceedings{pachet2013,
	address = {New York, NY, USA},
	series = {{CHI} '13},
	title = {Reflexive loopers for solo musical improvisation},
	isbn = {978-1-4503-1899-0},
	url = {https://doi.org/10.1145/2470654.2481303},
	doi = {10.1145/2470654.2481303},
	abstract = {Loop pedals are real-time samplers that playback audio played previously by a musician. Such pedals are routinely used for music practice or outdoor "busking". However, loop pedals always playback the same material, which can make performances monotonous and boring both to the musician and the audience, preventing their widespread uptake in professional concerts. In response, we propose a new approach to loop pedals that addresses this issue, which is based on an analytical multi-modal representation of the audio input. Instead of simply playing back prerecorded audio, our system enables real-time generation of an audio accompaniment reacting to what is currently being performed by the musician. By combining different modes of performance - e.g. bass line, chords, solo - from the musician and system automatically, solo musicians can perform duets or trios with themselves, without engendering the so-called canned (boringly repetitive and unresponsive) music effect of loop pedals. We describe the technology, based on supervised classification and concatenative synthesis, and then illustrate our approach on solo performances of jazz standards by guitar. We claim this approach opens up new avenues for concert performance.},
	urldate = {2021-04-08},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Pachet, François and Roy, Pierre and Moreira, Julian and d'Inverno, Mark},
	month = apr,
	year = {2013},
	keywords = {classification, loop pedals, music interaction, synthesis},
	pages = {2205--2208},
	file = {Full Text:/Users/Vincent/Zotero/storage/ICHCTJSP/Pachet et al. - 2013 - Reflexive loopers for solo musical improvisation.pdf:application/pdf}
}

@inproceedings{hamanaka2003,
	address = {San Francisco, CA, USA},
	series = {{IJCAI}'03},
	title = {A learning-based jam session system that imitates a player's personality model},
	abstract = {This paper describes a jam session system that enables a human player to interplay with virtual players which can imitate the player personality models of various human players. Previous systems have parameters that allow some alteration in the way virtual players react, but these systems cannot imitate human personalities. Our system can obtain three kinds of player personality models from a MIDI recording of a session in which that player participated - a reaction model, a phrase model, and a groove model. The reaction model is the characteristic way that a player reacts to other players, and it can be statistically learned from the relationship between the MIDI data of music the player listens to and the MIDI data of music improvised by that player. The phrase model is a set of player's characteristic phrases; it can be acquired through musical segmentation of a MIDI session recording by using Voronoi diagrams on a piano-roll. The groove model is a model that generates onset time deviation; it can be acquired by using a hidden Markov model. Experimental results show that the personality models of any player participating in a guitar trio session can be derived from a MIDI recording of that session.},
	urldate = {2021-04-08},
	booktitle = {Proceedings of the 18th international joint conference on {Artificial} intelligence},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Hamanaka, Masatoshi and Goto, Masataka and Asoh, Hideki and Otsu, Nobuyuki},
	month = aug,
	year = {2003},
	pages = {51--58}
}

@book{stowell,
	title = {Evaluation of live human-computer music-making: quantitative and qualitative approaches},
	shorttitle = {Evaluation of live human-computer music-making},
	abstract = {Live music-making using interactive systems is not completely amenable to traditional HCI evaluation metrics such as taskcompletion rates. In this paper we discuss quantitative and qualitative approaches which provide opportunities to evaluate the music-making interaction, accounting for aspects which cannot be directly measured or expressed numerically, yet which may be important for participants. We present case studies in the application of a qualitative method based on Discourse Analysis, and a quantitative method based on the Turing Test. We compare and contrast these methods with each other, and with other evaluation approaches used in the literature, and discuss factors affecting which evaluation methods are appropriate in a given context. Key words: Music, qualitative, quantitative 1.},
	author = {Stowell, D. and Robertson, A. and Bryan-kinns, N. and Plumbley, M. D.},
	file = {Citeseer - Snapshot:/Users/Vincent/Zotero/storage/JFZSY4JY/download.html:text/html;Citeseer - Full Text PDF:/Users/Vincent/Zotero/storage/KXC7EJLM/Stowell et al. - Evaluation of live human-computer music-making qu.pdf:application/pdf}
}

@inproceedings{ravikumar2017,
	address = {New York, NY, USA},
	series = {C\&amp;{C} '17},
	title = {Notational {Communication} with {Co}-creative {Systems}: {Studying} {Improvements} to {Musical} {Coordination}},
	isbn = {978-1-4503-4403-6},
	shorttitle = {Notational {Communication} with {Co}-creative {Systems}},
	url = {https://doi.org/10.1145/3059454.3078702},
	doi = {10.1145/3059454.3078702},
	abstract = {This research examines the impact of differences in extra-musical notational communication on a musician's co-creative experience with a music improvisation system. The method involves the development of musical improvisation agent technologies that support mixed-initiative communication and the use of mixed methods for studying the musician's co-creative engagement. The expected contributions are the findings about the impact of differences in extra-musical communication on the co-creative engagement with co-improvisation systems.},
	urldate = {2021-04-08},
	booktitle = {Proceedings of the 2017 {ACM} {SIGCHI} {Conference} on {Creativity} and {Cognition}},
	publisher = {Association for Computing Machinery},
	author = {Ravikumar, Prashanth Thattai},
	month = jun,
	year = {2017},
	keywords = {computational co-creativity, coordination, engagement, extra-musical communication, musical co-improvisation},
	pages = {518--523}
}

@inproceedings{thom2001,
	title = {Machine {Learning} {Techniques} for {Real}-time {Improvisational} {Solo} {Trading}},
	abstract = {This paper introduces a new melody generation scheme that enables customized interaction between a live, improvising musician and the computer. This scheme provides a method for integrating a model that was learned to describe the user's tonal, melodic-interval and -contour trends into a stochastic process that, when sampled, produces sequences that exhibit similar trends while also seamlessly integrating with the local environment. This algorithm's musical performance is evaluated both quantitatively, using traditional machine learning techniques, and qualitatively, exploring its behavior in the context of Bebop saxophonist Charlie Parker.},
	booktitle = {In {Proceedings} of {IS} 2001},
	author = {Thom, Belinda},
	year = {2001},
	file = {Citeseer - Snapshot:/Users/Vincent/Zotero/storage/JQ4WBMB2/summary.html:text/html;Citeseer - Full Text PDF:/Users/Vincent/Zotero/storage/MNDLGX6P/Thom - 2001 - Machine Learning Techniques for Real-time Improvis.pdf:application/pdf}
}

@misc{zotero-403,
	title = {feedback killer - {MaxMSP} {Forum} {\textbar} {Cycling} '74},
	url = {https://cycling74.com/forums/feedback-killer},
	abstract = {Hello,
Did anyone try to program a feedback killer with max? Actually, I'm doing a piece with real-time processing with 5 simple delays ...},
	language = {en},
	urldate = {2021-04-14},
	file = {Snapshot:/Users/Vincent/Zotero/storage/LKVMGJ5F/feedback-killer.html:text/html}
}
